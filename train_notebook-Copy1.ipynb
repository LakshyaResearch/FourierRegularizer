{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89d0bb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "==> Building model..\n",
      "==> Initialized Normalized-Fourier-regularizer lambda: 0.50\n"
     ]
    }
   ],
   "source": [
    "'''Train CIFAR10 with PyTorch.\n",
    "Reference: https://github.com/kuangliu/pytorch-cifar/\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "from resnet import ResNet18\n",
    "from utils import progress_bar\n",
    "from regularizers import FourierRegularizer,NormalizedFourierRegularizer\n",
    "\n",
    "\n",
    "###############################################\n",
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch CIFAR10 Training')\n",
    "parser.add_argument('--lr', default=0.1, type=float, help='learning rate')\n",
    "parser.add_argument('--regularizer', default=None, type=str, choices = ['Normalized_PSD','LSF', 'MSF', 'HSF', 'ASF'], help='Fourier Regularizer')\n",
    "parser.add_argument('--regularizer_lambda', default=0.5, type=float, help='regularizer weight')\n",
    "parser.add_argument('--resume', '-r', action='store_true', help='resume from checkpoint')\n",
    "\n",
    "\n",
    "args = parser.parse_args(args=[\"--regularizer\", \"Normalized_PSD\"])# pass an empty list to simulate command-line arguments\n",
    "################################################\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "# Data\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='/tmp', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='/tmp', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "# Model\n",
    "print('==> Building model..')\n",
    "\n",
    "net = ResNet18()\n",
    "net = net.to(device)\n",
    "\n",
    "\n",
    "# if device == 'cuda':\n",
    "\n",
    "#     print(\"aasd 1\")\n",
    "\n",
    "#     net = torch.nn.DataParallel(net, device_ids=[0])\n",
    "# #     net = torch.nn.parallel.DistributedDataParallel(net)\n",
    "#     print(\"aasd 2\")\n",
    "\n",
    "#     cudnn.benchmark = True\n",
    "\n",
    "if args.resume:\n",
    "    # Load checkpoint.\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
    "    checkpoint = torch.load('./checkpoint/ckpt.pth')\n",
    "    net.load_state_dict(checkpoint['net'])\n",
    "    best_acc = checkpoint['acc']\n",
    "    start_epoch = checkpoint['epoch']\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=args.lr,\n",
    "                      momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
    "\n",
    "\n",
    "if args.regularizer:\n",
    "    if args.regularizer=='Normalized_PSD':\n",
    "        args.regularizer = NormalizedFourierRegularizer(args.regularizer_lambda)\n",
    "        args.regularizer.mode='Normalized_PSD'\n",
    "    else:\n",
    "        args.regularizer = FourierRegularizer(args.regularizer, args.regularizer_lambda) # init regularizer\n",
    "#         args.regularizer.mode =''\n",
    "\n",
    "# Training\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12e90c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    train_reg_loss = 0\n",
    "    regularizer_loss = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        if args.regularizer:\n",
    "            inputs = inputs.requires_grad_(True)\n",
    "\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "#         print(\"aaaaaa\")\n",
    "        outputs = net(inputs)\n",
    "#         print(\"bbbbbb\")\n",
    "        \n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        if args.regularizer:\n",
    "            regularizer_loss =  args.regularizer(inputs, loss)\n",
    "            loss = loss + regularizer_loss\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_reg_loss += regularizer_loss\n",
    "        \n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Reg. Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                     % (train_loss/(batch_idx+1), train_reg_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "        \n",
    "        \n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                         % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "\n",
    "        if not os.path.isdir('checkpoint'):\n",
    "            os.mkdir('checkpoint')\n",
    "        \n",
    "        if args.regularizer:\n",
    "            torch.save(state, './checkpoint/ckpt_%s_%.2f.pth' % (args.regularizer.mode, args.regularizer_lambda))\n",
    "        else:\n",
    "            torch.save(state, './checkpoint/ckpt.pth')\n",
    "\n",
    "        best_acc = acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adda5e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucky\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\functional.py:505: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:3491.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [================================================================>]  Step: 195ms | Tot: 1m31s | Loss: 5.107 | Reg. Loss: 2.716 | Acc: 17.798% (8899/5000 391/391 1  \n",
      " [================================================================>]  Step: 11ms | Tot: 1s221ms | Loss: 1.993 | Acc: 25.760% (2576/1000 100/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 1\n",
      " [================================================================>]  Step: 159ms | Tot: 1m31s | Loss: 4.326 | Reg. Loss: 2.366 | Acc: 25.948% (12974/5000 391/391   \n",
      " [================================================================>]  Step: 10ms | Tot: 1s241ms | Loss: 1.841 | Acc: 31.460% (3146/1000 100/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 2\n",
      " [================================================================>]  Step: 161ms | Tot: 1m31s | Loss: 4.197 | Reg. Loss: 2.351 | Acc: 31.502% (15751/5000 391/391 1  \n",
      " [================================================================>]  Step: 11ms | Tot: 1s125ms | Loss: 1.754 | Acc: 35.380% (3538/1000 100/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 3\n",
      " [================================================================>]  Step: 160ms | Tot: 1m32s | Loss: 4.096 | Reg. Loss: 2.342 | Acc: 35.828% (17914/5000 391/391 1  \n",
      " [================================================================>]  Step: 11ms | Tot: 1s140ms | Loss: 1.668 | Acc: 40.140% (4014/1000 100/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 4\n",
      " [================================================================>]  Step: 159ms | Tot: 1m32s | Loss: 4.007 | Reg. Loss: 2.338 | Acc: 39.502% (19751/5000 391/391 1  \n",
      " [================================================================>]  Step: 10ms | Tot: 1s142ms | Loss: 1.567 | Acc: 42.610% (4261/1000 100/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 5\n",
      " [================================================================>]  Step: 159ms | Tot: 1m32s | Loss: 3.948 | Reg. Loss: 2.328 | Acc: 41.170% (20585/5000 391/391 1  \n",
      " [================================================================>]  Step: 10ms | Tot: 1s149ms | Loss: 1.638 | Acc: 42.020% (4202/1000 100/100 \n",
      "\n",
      "Epoch: 6\n",
      " [================================================================>]  Step: 158ms | Tot: 1m31s | Loss: 3.923 | Reg. Loss: 2.334 | Acc: 42.470% (21235/5000 391/391 1  \n",
      " [================================================================>]  Step: 10ms | Tot: 1s202ms | Loss: 1.585 | Acc: 42.130% (4213/1000 100/100 \n",
      "\n",
      "Epoch: 7\n",
      " [================================================================>]  Step: 161ms | Tot: 1m32s | Loss: 3.938 | Reg. Loss: 2.354 | Acc: 42.418% (21209/5000 391/391 1  \n",
      " [================================================================>]  Step: 10ms | Tot: 1s148ms | Loss: 1.666 | Acc: 39.900% (3990/1000 100/100 \n",
      "\n",
      "Epoch: 8\n",
      " [================================================================>]  Step: 159ms | Tot: 1m32s | Loss: 3.967 | Reg. Loss: 2.363 | Acc: 41.854% (20927/5000 391/391 1  \n",
      " [================================================================>]  Step: 12ms | Tot: 1s155ms | Loss: 1.582 | Acc: 42.350% (4235/1000 100/100 \n",
      "\n",
      "Epoch: 9\n",
      " [================================================================>]  Step: 162ms | Tot: 1m32s | Loss: 4.027 | Reg. Loss: 2.405 | Acc: 41.214% (20607/5000 391/391 1  \n",
      " [================================================================>]  Step: 10ms | Tot: 1s138ms | Loss: 1.690 | Acc: 40.530% (4053/1000 100/100 \n",
      "\n",
      "Epoch: 10\n",
      " [================================================================>]  Step: 158ms | Tot: 1m32s | Loss: 3.988 | Reg. Loss: 2.386 | Acc: 41.830% (20915/5000 391/391 1  \n",
      " [================================================================>]  Step: 11ms | Tot: 1s141ms | Loss: 1.614 | Acc: 41.700% (4170/1000 100/100 \n",
      "\n",
      "Epoch: 11\n",
      " [================================================================>]  Step: 167ms | Tot: 1m32s | Loss: 3.962 | Reg. Loss: 2.379 | Acc: 42.764% (21382/5000 391/391 1  \n",
      " [================================================================>]  Step: 11ms | Tot: 1s150ms | Loss: 1.783 | Acc: 36.800% (3680/1000 100/100 \n",
      "\n",
      "Epoch: 12\n",
      " [================================================================>]  Step: 173ms | Tot: 1m35s | Loss: 3.951 | Reg. Loss: 2.383 | Acc: 43.170% (21585/5000 391/391 1  \n",
      " [================================================================>]  Step: 12ms | Tot: 1s360ms | Loss: 1.537 | Acc: 44.160% (4416/1000 100/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 13\n",
      " [================================================================>]  Step: 171ms | Tot: 1m37s | Loss: 3.992 | Reg. Loss: 2.403 | Acc: 42.200% (21100/5000 391/391 1  \n",
      " [================================================================>]  Step: 11ms | Tot: 1s163ms | Loss: 1.643 | Acc: 40.250% (4025/1000 100/100 \n",
      "\n",
      "Epoch: 14\n",
      " [================================================================>]  Step: 172ms | Tot: 1m39s | Loss: 3.995 | Reg. Loss: 2.414 | Acc: 42.734% (21367/5000 391/391 1  \n",
      " [================================================================>]  Step: 10ms | Tot: 1s171ms | Loss: 1.742 | Acc: 36.700% (3670/1000 100/100 \n",
      "\n",
      "Epoch: 15\n",
      " [================================================>................]  Step: 236ms | Tot: 1m11s | Loss: 4.046 | Reg. Loss: 2.445 | Acc: 41.865% (15701/3750 293/391 1  \r"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, start_epoch+150):\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889bf4bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch] *",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
